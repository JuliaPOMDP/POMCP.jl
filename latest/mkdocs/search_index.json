{
    "docs": [
        {
            "location": "/", 
            "text": "POMCP.jl\n\n\nThe Partially Observable Monte Carlo Planning (POMCP) online solver for POMDPs.jl.\n\n\nDescribed in\n\n\nSilver, D., \n Veness, J. (2010). Monte-Carlo Planning in Large POMDPs. In \nAdvances in neural information processing systems\n (pp. 2164\u20132172). Retrieved from http://discovery.ucl.ac.uk/1347369/\n\n\n\n\nInstallation\n\n\nPkg.clone(\nhttps://github.com/sisl/POMCP.jl.git\n)\n\n\n\n\n\n\nDocumentation\n\n\nThis implementation of the POMCP solver may be used to solve POMDPs defined according to the \nPOMDPs.jl\n interface. Note that this is an online solver, so the computation is carried out as the simulation is running (simulations take a long time, but \nsolve\n takes no time).\n\n\nFor a usage example, see the \nBasic Usage\n notebook. For some more (poorly documented) examples, see the \nSanity Checks\n notebook.\n\n\nBehavior is controlled through two mechanisms: \nsolver options\n and \nmethod specializations\n.\n\n\nThere is also an interactive \nsearch tree visualizer\n.", 
            "title": "Home"
        }, 
        {
            "location": "/#pomcpjl", 
            "text": "The Partially Observable Monte Carlo Planning (POMCP) online solver for POMDPs.jl.  Described in  Silver, D.,   Veness, J. (2010). Monte-Carlo Planning in Large POMDPs. In  Advances in neural information processing systems  (pp. 2164\u20132172). Retrieved from http://discovery.ucl.ac.uk/1347369/", 
            "title": "POMCP.jl"
        }, 
        {
            "location": "/#installation", 
            "text": "Pkg.clone( https://github.com/sisl/POMCP.jl.git )", 
            "title": "Installation"
        }, 
        {
            "location": "/#documentation", 
            "text": "This implementation of the POMCP solver may be used to solve POMDPs defined according to the  POMDPs.jl  interface. Note that this is an online solver, so the computation is carried out as the simulation is running (simulations take a long time, but  solve  takes no time).  For a usage example, see the  Basic Usage  notebook. For some more (poorly documented) examples, see the  Sanity Checks  notebook.  Behavior is controlled through two mechanisms:  solver options  and  method specializations .  There is also an interactive  search tree visualizer .", 
            "title": "Documentation"
        }, 
        {
            "location": "/solver/", 
            "text": "Solver\n\n\nSolver Options are controlled through the POMCPSolver keyword constructor:\n\n\n#\n\n\nPOMCP.POMCPSolver\n \n \nMethod\n.\n\n\nConstructor for the POMCP Solver\n\n\nPOMCPSolver properties are:\n\n\n\n\neps\n - Rollout simulations are terminated once the discount factor raised to the current step power is below this (see paper). default: 0.01\n\n\nc\n - UCB tuning parameter (see paper). default: 1\n\n\ntree_queries\n - Number of nodes created in the tree per action decision.\n\n\nrng\n - Random number generator.\n\n\nupdater\n - A \nPOMDPs.Updater\n to be used to update the belief within the belief tree. By default the particle filter described in the paper will be used.\n\n\nvalue_estimate_method\n - Either \n:value\n to use the \nPOMDPs.value()\n function or \nrollout\n to use a rollout simulation.\n\n\nrollout_solver\n - This should be a \nPOMDPs.Solver\n or \nPOMDPs.Policy\n that will be used in rollout simulations. If it is a \nSolver\n, \nsolve\n will be called to determine the rollout policy. By default a random policy is used.\n\n\nrollout_updater\n - The belief updater that will be used in the rollout simulations. default: \nupdater(rollout_policy)\n.\n\n\nnum_sparse_actions\n - If only a limited number of actions are to be considered, set this. If it is 0, all actions will be considered.", 
            "title": "Solver"
        }, 
        {
            "location": "/solver/#solver", 
            "text": "Solver Options are controlled through the POMCPSolver keyword constructor:  #  POMCP.POMCPSolver     Method .  Constructor for the POMCP Solver  POMCPSolver properties are:   eps  - Rollout simulations are terminated once the discount factor raised to the current step power is below this (see paper). default: 0.01  c  - UCB tuning parameter (see paper). default: 1  tree_queries  - Number of nodes created in the tree per action decision.  rng  - Random number generator.  updater  - A  POMDPs.Updater  to be used to update the belief within the belief tree. By default the particle filter described in the paper will be used.  value_estimate_method  - Either  :value  to use the  POMDPs.value()  function or  rollout  to use a rollout simulation.  rollout_solver  - This should be a  POMDPs.Solver  or  POMDPs.Policy  that will be used in rollout simulations. If it is a  Solver ,  solve  will be called to determine the rollout policy. By default a random policy is used.  rollout_updater  - The belief updater that will be used in the rollout simulations. default:  updater(rollout_policy) .  num_sparse_actions  - If only a limited number of actions are to be considered, set this. If it is 0, all actions will be considered.", 
            "title": "Solver"
        }, 
        {
            "location": "/methods/", 
            "text": "Methods for Specialization\n\n\nThe following methods can be overridden to change the behavior of the solver:\n\n\n#\n\n\nPOMCP.extract_belief\n \n \nMethod\n.\n\n\nextract_belief(rollout_updater::POMDPs.Updater, node::ObsNode) = initialize_belief(rollout_updater, node.B)\n\n\n\n\nReturn a belief compatible with the \nrollout_updater\n from the belief in \nnode\n.\n\n\nWhen a rollout simulation is started, this method is used to create the initial belief (compatible with \nrollout_updater\n) based on the appropriate \nBeliefNode\n at the edge of the tree. By overriding this, a belief can be constructed based on the entire tree or entire state-action history. If this is not overriden, by default it will use initialize_belief on the belief associated with the node directly, i.e. \nPOMDPs.initialize_belief(rollout_updater, node.B)\n.\n\n\n#\n\n\nPOMCP.init_V\n \n \nMethod\n.\n\n\ninit_V(problem::POMDPs.POMDP, h::BeliefNode, action)\n\n\n\n\nReturn the initial value (V) associated with a new action node when it is created. This can be used in concert with \ninit_N\n to incorporate prior experience into the solver.\n\n\n#\n\n\nPOMCP.init_N\n \n \nMethod\n.\n\n\ninit_N(problem::POMDPs.POMDP, h::BeliefNode, action)\n\n\n\n\nReturn the initial number of queries (N) associated with a new action node when it is created.\n\n\n#\n\n\nPOMCP.sparse_actions\n \n \nMethod\n.\n\n\nsparse_actions(pomcp::POMCPPolicy, pomdp::POMDPs.POMDP, h::BeliefNode, num_actions::Int)\n\n\n\n\nReturn an iterable object containing no more than \nnum_actions\n actions to be considered at the current node.\n\n\nOverride this if you want to choose specific actions (you can override based on the POMDP type at the node level, or the belief type). If only a limited number of actions are to be considered, this function will be used to generate that set of actions. By default, it simply returns a random sampling of actions from the action space generated by \nPOMDPs.actions\n.\n\n\nIf your problem has a continuous action space, you will want to override this to try a sensible set of action samples.\n\n\n#\n\n\nPOMCP.estimate_value\n \n \nMethod\n.\n\n\nestimate_value(pomcp::POMCPPolicy, problem::POMDPs.POMDP, start_state, h::BeliefNode)\n\n\n\n\nReturn an initial unbiased estimate of the value at belief node h.\n\n\nBy default this runs a rollout simulation", 
            "title": "Methods"
        }, 
        {
            "location": "/methods/#methods-for-specialization", 
            "text": "The following methods can be overridden to change the behavior of the solver:  #  POMCP.extract_belief     Method .  extract_belief(rollout_updater::POMDPs.Updater, node::ObsNode) = initialize_belief(rollout_updater, node.B)  Return a belief compatible with the  rollout_updater  from the belief in  node .  When a rollout simulation is started, this method is used to create the initial belief (compatible with  rollout_updater ) based on the appropriate  BeliefNode  at the edge of the tree. By overriding this, a belief can be constructed based on the entire tree or entire state-action history. If this is not overriden, by default it will use initialize_belief on the belief associated with the node directly, i.e.  POMDPs.initialize_belief(rollout_updater, node.B) .  #  POMCP.init_V     Method .  init_V(problem::POMDPs.POMDP, h::BeliefNode, action)  Return the initial value (V) associated with a new action node when it is created. This can be used in concert with  init_N  to incorporate prior experience into the solver.  #  POMCP.init_N     Method .  init_N(problem::POMDPs.POMDP, h::BeliefNode, action)  Return the initial number of queries (N) associated with a new action node when it is created.  #  POMCP.sparse_actions     Method .  sparse_actions(pomcp::POMCPPolicy, pomdp::POMDPs.POMDP, h::BeliefNode, num_actions::Int)  Return an iterable object containing no more than  num_actions  actions to be considered at the current node.  Override this if you want to choose specific actions (you can override based on the POMDP type at the node level, or the belief type). If only a limited number of actions are to be considered, this function will be used to generate that set of actions. By default, it simply returns a random sampling of actions from the action space generated by  POMDPs.actions .  If your problem has a continuous action space, you will want to override this to try a sensible set of action samples.  #  POMCP.estimate_value     Method .  estimate_value(pomcp::POMCPPolicy, problem::POMDPs.POMDP, start_state, h::BeliefNode)  Return an initial unbiased estimate of the value at belief node h.  By default this runs a rollout simulation", 
            "title": "Methods for Specialization"
        }, 
        {
            "location": "/visualization/", 
            "text": "Tree Visualization\n\n\nInteractive visualization of the MCTS tree is available in Jupyter notebooks. Run the Display Tree notebook in the notebooks folder for a demonstration (It does not display correctly in the notebook hosted on github; it must be run on a local machine).\n\n\nIn order to display a tree, create a POMCPTreeVisualizer with any BeliefNode. If the last line of a cell in a Jupyter notebook returns a POMCPTreeVisualizer, the output cell will be populated with html and javascript that display the tree. See the documentation for MCTS.jl for more detailed information about the tree and instructions describing how to customize its appearance. In particular, the \nnode_tag\n and \ntooltip_tag\n functions can be overridden to customize how actions and observations are displayed.", 
            "title": "Visualization"
        }, 
        {
            "location": "/visualization/#tree-visualization", 
            "text": "Interactive visualization of the MCTS tree is available in Jupyter notebooks. Run the Display Tree notebook in the notebooks folder for a demonstration (It does not display correctly in the notebook hosted on github; it must be run on a local machine).  In order to display a tree, create a POMCPTreeVisualizer with any BeliefNode. If the last line of a cell in a Jupyter notebook returns a POMCPTreeVisualizer, the output cell will be populated with html and javascript that display the tree. See the documentation for MCTS.jl for more detailed information about the tree and instructions describing how to customize its appearance. In particular, the  node_tag  and  tooltip_tag  functions can be overridden to customize how actions and observations are displayed.", 
            "title": "Tree Visualization"
        }
    ]
}