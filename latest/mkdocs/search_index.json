{
    "docs": [
        {
            "location": "/", 
            "text": "POMCP.jl\n\n\nThe Partially Observable Monte Carlo Planning (POMCP) online solver for POMDPs.jl.\n\n\nDescribed in\n\n\nSilver, D., \n Veness, J. (2010). Monte-Carlo Planning in Large POMDPs. In \nAdvances in neural information processing systems\n (pp. 2164\u20132172). Retrieved from http://discovery.ucl.ac.uk/1347369/\n\n\n\n\nInstallation\n\n\nSee \nREADME.md\n on the \nGithub repo\n.\n\n\n\n\nDocumentation\n\n\n\n\nBasics\n\n\nThis implementation of the POMCP solver may be used to solve POMDPs defined according to the \nPOMDPs.jl\n interface. Note that this is an online solver, so the computation is carried out as the simulation is running (simulations take a long time, but \nsolve\n takes no time).\n\n\nFor a usage example, see the \nBasic Usage\n notebook. For some more (poorly documented) examples, see the \nSanity Checks\n notebook.\n\n\nBehavior is controlled through two mechanisms: \nsolver options\n and \nmethod specializations\n.\n\n\n\n\nBelief Updates\n\n\nBy default, POMCP uses an unweighted particle filter for belief updates as discussed in the original paper describing it. However, this implementation can use any Updater to keep track of the belief. A notebook describing the various belief updater options and features can be viewed here: \nhttp://nbviewer.jupyter.org/github/JuliaPOMDP/POMCP.jl/blob/master/notebooks/Belief_and_Particle_Filter_Options.ipynb\n\n\n\n\nTree Visualization\n\n\nInteractive visualization of the POMCP tree is available in Jupyter notebooks. Run the Display Tree notebook in the notebooks folder for a demonstration, or view it here: \nhttp://nbviewer.jupyter.org/github/JuliaPOMDP/POMCP.jl/blob/master/notebooks/Display_Tree.ipynb\n\n\nIn order to display a tree, create a POMCPTreeVisualizer with any BeliefNode. If the last line of a cell in a Jupyter notebook returns a POMCPTreeVisualizer, the output cell will be populated with html and javascript that display the tree. See the documentation for MCTS.jl for more detailed information about the tree and instructions describing how to customize its appearance. In particular, the \nnode_tag\n and \ntooltip_tag\n functions can be overridden to customize how actions and observations are displayed.", 
            "title": "Home"
        }, 
        {
            "location": "/#pomcpjl", 
            "text": "The Partially Observable Monte Carlo Planning (POMCP) online solver for POMDPs.jl.  Described in  Silver, D.,   Veness, J. (2010). Monte-Carlo Planning in Large POMDPs. In  Advances in neural information processing systems  (pp. 2164\u20132172). Retrieved from http://discovery.ucl.ac.uk/1347369/", 
            "title": "POMCP.jl"
        }, 
        {
            "location": "/#installation", 
            "text": "See  README.md  on the  Github repo .", 
            "title": "Installation"
        }, 
        {
            "location": "/#documentation", 
            "text": "", 
            "title": "Documentation"
        }, 
        {
            "location": "/#basics", 
            "text": "This implementation of the POMCP solver may be used to solve POMDPs defined according to the  POMDPs.jl  interface. Note that this is an online solver, so the computation is carried out as the simulation is running (simulations take a long time, but  solve  takes no time).  For a usage example, see the  Basic Usage  notebook. For some more (poorly documented) examples, see the  Sanity Checks  notebook.  Behavior is controlled through two mechanisms:  solver options  and  method specializations .", 
            "title": "Basics"
        }, 
        {
            "location": "/#belief-updates", 
            "text": "By default, POMCP uses an unweighted particle filter for belief updates as discussed in the original paper describing it. However, this implementation can use any Updater to keep track of the belief. A notebook describing the various belief updater options and features can be viewed here:  http://nbviewer.jupyter.org/github/JuliaPOMDP/POMCP.jl/blob/master/notebooks/Belief_and_Particle_Filter_Options.ipynb", 
            "title": "Belief Updates"
        }, 
        {
            "location": "/#tree-visualization", 
            "text": "Interactive visualization of the POMCP tree is available in Jupyter notebooks. Run the Display Tree notebook in the notebooks folder for a demonstration, or view it here:  http://nbviewer.jupyter.org/github/JuliaPOMDP/POMCP.jl/blob/master/notebooks/Display_Tree.ipynb  In order to display a tree, create a POMCPTreeVisualizer with any BeliefNode. If the last line of a cell in a Jupyter notebook returns a POMCPTreeVisualizer, the output cell will be populated with html and javascript that display the tree. See the documentation for MCTS.jl for more detailed information about the tree and instructions describing how to customize its appearance. In particular, the  node_tag  and  tooltip_tag  functions can be overridden to customize how actions and observations are displayed.", 
            "title": "Tree Visualization"
        }, 
        {
            "location": "/solver/", 
            "text": "Solver\n\n\nSolver options are controlled through the solver keyword constructor:\n\n\n#\n\n\nPOMCP.POMCPSolver\n \n \nMethod\n.\n\n\nConstructor for the POMCP Solver\n\n\nPOMCPSolver properties are:\n\n\n\n\neps\n - Rollout simulations are terminated once the discount factor raised to the current step power is below this (see paper). default: 0.01\n\n\nmax_depth\n - Rollout simulations and tree expansion are terminated at this depth. default: typemax(Int)\n\n\nc\n - UCB tuning parameter (see paper). default: 1\n\n\ntree_queries\n - Number of nodes created in the tree per action decision.\n\n\nrng\n - Random number generator.\n\n\nnode_belief_updater\n - A \nPOMDPs.Updater\n to be used to update the belief in the nodes of the belief tree. By default the particle filter described in the paper will be used.\n\n\nvalue_estimate_method\n - Either \n:value\n to use the \nPOMDPs.value()\n function or \n:rollout\n to use a rollout simulation.\n\n\nrollout_solver\n - This should be a \nPOMDPs.Solver\n or \nPOMDPs.Policy\n that will be used in rollout simulations. If it is a \nSolver\n, \nsolve\n will be called to determine the rollout policy. By default a random policy is used.\n\n\nnum_sparse_actions\n - If only a limited number of actions are to be considered, set this. If it is 0, all actions will be considered.\n\n\n\n\n#\n\n\nPOMCP.POMCPDPWSolver\n \n \nMethod\n.\n\n\nConstructor for the POMCP DPW Solver\n\n\nPOMCPSolver properties are:\n\n\n\n\neps\n - Rollout simulations are terminated once the discount factor raised to the current step power is below this (see paper). default: 0.01\n\n\nmax_depth\n - Rollout simulations and tree expansion are terminated at this depth. default: typemax(Int)\n\n\nc\n - UCB tuning parameter (see paper). default: 1\n\n\ntree_queries\n - Number of nodes created in the tree per action decision.\n\n\nrng\n - Random number generator.\n\n\nnode_belief_updater\n - A \nPOMDPs.Updater\n to be used to update the belief in the nodes of the belief tree. By default the particle filter described in the paper will be used.\n\n\nvalue_estimate_method\n - Either \n:value\n to use the \nPOMDPs.value()\n function or \n:rollout\n to use a rollout simulation.\n\n\nrollout_solver\n - This should be a \nPOMDPs.Solver\n or \nPOMDPs.Policy\n that will be used in rollout simulations. If it is a \nSolver\n, \nsolve\n will be called to determine the rollout policy. By default a random policy is used.\n\n\nalpha_observation\n - Exponent parameter for progressive widening of the number of observation nodes. default: 0.5\n\n\nk_observation\n - Linear parameter for progressive widening of the number of observation nodes. default: 10.0\n\n\nalpha_action\n - Exponent parameter for progressive widening of the number of action nodes. default: 0.5\n\n\nk_action\n - Linear parameter for progressive widening of the number of observation nodes. default: 10.0\n\n\ngen\n - \nActionGenerator\n for specifiying which new action should be tried when the node is widened. See MCTS.jl for the \nActionGenerator\n definition\n\n\n\n\nFor more information on the k and alpha parameters, see Cou\u00ebtoux, A., Hoock, J.-B., Sokolovska, N., Teytaud, O., \n Bonnard, N. (2011). Continuous Upper Confidence Trees. In Learning and Intelligent Optimization. Rome, Italy. Retrieved from http://link.springer.com/chapter/10.1007/978-3-642-25566-3_32", 
            "title": "Solver"
        }, 
        {
            "location": "/solver/#solver", 
            "text": "Solver options are controlled through the solver keyword constructor:  #  POMCP.POMCPSolver     Method .  Constructor for the POMCP Solver  POMCPSolver properties are:   eps  - Rollout simulations are terminated once the discount factor raised to the current step power is below this (see paper). default: 0.01  max_depth  - Rollout simulations and tree expansion are terminated at this depth. default: typemax(Int)  c  - UCB tuning parameter (see paper). default: 1  tree_queries  - Number of nodes created in the tree per action decision.  rng  - Random number generator.  node_belief_updater  - A  POMDPs.Updater  to be used to update the belief in the nodes of the belief tree. By default the particle filter described in the paper will be used.  value_estimate_method  - Either  :value  to use the  POMDPs.value()  function or  :rollout  to use a rollout simulation.  rollout_solver  - This should be a  POMDPs.Solver  or  POMDPs.Policy  that will be used in rollout simulations. If it is a  Solver ,  solve  will be called to determine the rollout policy. By default a random policy is used.  num_sparse_actions  - If only a limited number of actions are to be considered, set this. If it is 0, all actions will be considered.   #  POMCP.POMCPDPWSolver     Method .  Constructor for the POMCP DPW Solver  POMCPSolver properties are:   eps  - Rollout simulations are terminated once the discount factor raised to the current step power is below this (see paper). default: 0.01  max_depth  - Rollout simulations and tree expansion are terminated at this depth. default: typemax(Int)  c  - UCB tuning parameter (see paper). default: 1  tree_queries  - Number of nodes created in the tree per action decision.  rng  - Random number generator.  node_belief_updater  - A  POMDPs.Updater  to be used to update the belief in the nodes of the belief tree. By default the particle filter described in the paper will be used.  value_estimate_method  - Either  :value  to use the  POMDPs.value()  function or  :rollout  to use a rollout simulation.  rollout_solver  - This should be a  POMDPs.Solver  or  POMDPs.Policy  that will be used in rollout simulations. If it is a  Solver ,  solve  will be called to determine the rollout policy. By default a random policy is used.  alpha_observation  - Exponent parameter for progressive widening of the number of observation nodes. default: 0.5  k_observation  - Linear parameter for progressive widening of the number of observation nodes. default: 10.0  alpha_action  - Exponent parameter for progressive widening of the number of action nodes. default: 0.5  k_action  - Linear parameter for progressive widening of the number of observation nodes. default: 10.0  gen  -  ActionGenerator  for specifiying which new action should be tried when the node is widened. See MCTS.jl for the  ActionGenerator  definition   For more information on the k and alpha parameters, see Cou\u00ebtoux, A., Hoock, J.-B., Sokolovska, N., Teytaud, O.,   Bonnard, N. (2011). Continuous Upper Confidence Trees. In Learning and Intelligent Optimization. Rome, Italy. Retrieved from http://link.springer.com/chapter/10.1007/978-3-642-25566-3_32", 
            "title": "Solver"
        }, 
        {
            "location": "/methods/", 
            "text": "Methods for Specialization\n\n\nThe following methods can be overridden to change the behavior of the solver:\n\n\n#\n\n\nPOMCP.extract_belief\n \n \nFunction\n.\n\n\nextract_belief(rollout_updater::POMDPs.Updater, node::ObsNode) = initialize_belief(rollout_updater, node.B)\n\n\n\n\nReturn a belief compatible with the \nrollout_updater\n from the belief in \nnode\n.\n\n\nWhen a rollout simulation is started, this method is used to create the initial belief (compatible with \nrollout_updater\n) based on the appropriate \nBeliefNode\n at the edge of the tree. By overriding this, a belief can be constructed based on the entire tree or entire observation-action history. If this is not overriden, by default it will use initialize_belief on the belief associated with the node directly, i.e. \nPOMDPs.initialize_belief(rollout_updater, node.B)\n.\n\n\n#\n\n\nPOMCP.init_V\n \n \nFunction\n.\n\n\ninit_V(problem::POMDPs.POMDP, h::BeliefNode, action)\n\n\n\n\nReturn the initial value (V) associated with a new action node when it is created. This can be used in concert with \ninit_N\n to incorporate prior experience into the solver.\n\n\n#\n\n\nPOMCP.init_N\n \n \nFunction\n.\n\n\ninit_N(problem::POMDPs.POMDP, h::BeliefNode, action)\n\n\n\n\nReturn the initial number of queries (N) associated with a new action node when it is created.\n\n\n#\n\n\nPOMCP.sparse_actions\n \n \nFunction\n.\n\n\nsparse_actions(pomcp::POMCPPlanner, pomdp::POMDPs.POMDP, h::BeliefNode, num_actions::Int)\n\n\n\n\nReturn an iterable object containing no more than \nnum_actions\n actions to be considered at the current node.\n\n\nOverride this if you want to choose specific actions (you can override based on the POMDP type at the node level, or the belief type). If only a limited number of actions are to be considered, this function will be used to generate that set of actions. By default, it simply returns a random sampling of actions from the action space generated by \nPOMDPs.actions\n.\n\n\nIf your problem has a continuous action space, you will want to override this to try a sensible set of action samples.\n\n\n#\n\n\nPOMCP.estimate_value\n \n \nFunction\n.\n\n\nestimate_value(pomcp::POMCPPlanner, problem::POMDPs.POMDP, start_state, h::BeliefNode)\n\n\n\n\nReturn an initial unbiased estimate of the value at belief node h.\n\n\nBy default this runs a rollout simulation", 
            "title": "Methods"
        }, 
        {
            "location": "/methods/#methods-for-specialization", 
            "text": "The following methods can be overridden to change the behavior of the solver:  #  POMCP.extract_belief     Function .  extract_belief(rollout_updater::POMDPs.Updater, node::ObsNode) = initialize_belief(rollout_updater, node.B)  Return a belief compatible with the  rollout_updater  from the belief in  node .  When a rollout simulation is started, this method is used to create the initial belief (compatible with  rollout_updater ) based on the appropriate  BeliefNode  at the edge of the tree. By overriding this, a belief can be constructed based on the entire tree or entire observation-action history. If this is not overriden, by default it will use initialize_belief on the belief associated with the node directly, i.e.  POMDPs.initialize_belief(rollout_updater, node.B) .  #  POMCP.init_V     Function .  init_V(problem::POMDPs.POMDP, h::BeliefNode, action)  Return the initial value (V) associated with a new action node when it is created. This can be used in concert with  init_N  to incorporate prior experience into the solver.  #  POMCP.init_N     Function .  init_N(problem::POMDPs.POMDP, h::BeliefNode, action)  Return the initial number of queries (N) associated with a new action node when it is created.  #  POMCP.sparse_actions     Function .  sparse_actions(pomcp::POMCPPlanner, pomdp::POMDPs.POMDP, h::BeliefNode, num_actions::Int)  Return an iterable object containing no more than  num_actions  actions to be considered at the current node.  Override this if you want to choose specific actions (you can override based on the POMDP type at the node level, or the belief type). If only a limited number of actions are to be considered, this function will be used to generate that set of actions. By default, it simply returns a random sampling of actions from the action space generated by  POMDPs.actions .  If your problem has a continuous action space, you will want to override this to try a sensible set of action samples.  #  POMCP.estimate_value     Function .  estimate_value(pomcp::POMCPPlanner, problem::POMDPs.POMDP, start_state, h::BeliefNode)  Return an initial unbiased estimate of the value at belief node h.  By default this runs a rollout simulation", 
            "title": "Methods for Specialization"
        }
    ]
}